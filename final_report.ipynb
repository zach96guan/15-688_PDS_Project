{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York City Yellow Taxi Trip Data in First Quarter of 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import patsy\n",
    "import xgboost as xgb\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.stats import lognorm\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, optimizers\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "\n",
    "Similar to the homework about web scraping with Yelp API, we write function to extract the JSON format data from websites. We set right values of limit and offset parameters to get all queried records, and return Pandas dataframe for the conveniece of data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 Yellow Taxi Trip Data\n",
    "def extract_data(url, query):\n",
    "    new_url = url + query\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    i = 0\n",
    "    cur_page = new_url + \"&$limit=1000&$offset=\" + str(i)\n",
    "    cur_data = pd.read_json(cur_page)\n",
    "    while len(cur_data) >= 1000:\n",
    "        df = df.append(cur_data, sort=True)\n",
    "        i += 1000\n",
    "        cur_page = new_url + \"&$limit=1000&$offset=\" + str(i)\n",
    "        cur_data = pd.read_json(cur_page)\n",
    "        \n",
    "        if i % 4e5 == 0:\n",
    "            print(\"{} records have been extracted at {}.\".format(i, datetime.now().time()))\n",
    "    \n",
    "    if len(cur_data) > 0:\n",
    "        df = df.append(cur_data, sort=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to save/read data from generated .csv files. For the get data function, it takes around one and half an hour to successfully extract all NYC yellow taxi trip data in the first quarter of 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(df, fileName=\"./data/myData.csv\"):\n",
    "    df.to_csv(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_csv(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    url = \"https://data.cityofnewyork.us/resource/2upf-qytp.json?\"\n",
    "    # pickup time: Jan. ~ June, pickup location: <= 50\n",
    "    query = \"$where=tpep_pickup_datetime between '2019-01-01' and '2019-04-01' and PULocationID<=50\"\n",
    "    query = query.replace(\" \", \"%20\")\n",
    "    print(url + query)  # first page\n",
    "\n",
    "    print(\"Start time: \", datetime.now().time())\n",
    "    df = extract_data(url, query)\n",
    "    print(\"End time: \", datetime.now().time())\n",
    "\n",
    "    save_to_csv(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "On the NYC Open Data website, there is an [attachment](https://data.cityofnewyork.us/api/views/2upf-qytp/files/4a7a18af-bfc8-43d1-8a2e-faa503f75eb5?download=true&filename=data_dictionary_trip_records_yellow.pdf) that talks about the columns information about the dataset, listed as the following table.\n",
    "\n",
    "\n",
    "| **Column Name** | **Information** | \n",
    "|----------|:-------------|\n",
    "| VendorID | A code indicating the LPEP provider that provided the record. (1=Creative Mobile Technologies, LLC 2=VeriFone Inc)|\n",
    "| tpep_pickup_datetime | The date and time when the meter was engaged.|\n",
    "| tpep_dropoff_datetime | The date and time when the meter was disengaged.|\n",
    "| Passenger_count| The number of passengers in the vehicle.|\n",
    "| Trip_distance | The elapsed trip distance in miles reported by the taximeter.|\n",
    "| PULocationID | TLC Taxi Zone in which the taximeter was engaged|\n",
    "| DOLocationID | TLC Taxi Zone in which the taximeter was disengaged|\n",
    "| RateCodeID | The final rate code in effect at the end of the trip. (1=Standard rate, 2=JFK, 3=Newark, 4=Nassau or Westchester, 5=Negotiated fare, 6=Group ride)|  \n",
    "| Payment_type| A numeric code signifying how the passenger paid for the trip. (1=Credit card, 2=Cash 3=No charge, 4=Dispute, 5=Unknown, 6=Voided trip)|\n",
    "| Fare_amount | The time-and-distance fare calculated by the meter.|\n",
    "|Extra | Miscellaneous extras and surcharges. Currently, this only includes the \\$0.50 and \\$1.0 rush hour and overnight charges.|\n",
    "| MTA_tax | \\$0.50 MTA tax that is automatically triggered based on the metered rate in use.|\n",
    "| Improvement_surcharge | \\$0.30 improvement surcharge assessed on hailed trips at the flag drop.|\n",
    "| Tip amount | This field is automatically populated for credit card tips. Cash tips are not included.|\n",
    "| Tolls_amount | Total amount of all tolls paid in trip.|\n",
    "| Total_amount | The total amount charged to passengers. Does not include cash tips.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "For the initial stage of data cleaning, we define several functions to preprocess the raw dataset. Afterwards we will use the output \"myData.csv\" to do data analysis, in seek of some data insights about what model we could possibly utilize in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_columns(df):\n",
    "    # get rid of useless columns\n",
    "    df = df.drop(columns=[\"congestion_surcharge\", \"store_and_fwd_flag\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_timestamp(df):\n",
    "    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df[\"tpep_dropoff_datetime\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_duration_colum(df):\n",
    "    # create \"duration\" column from pickup and dropoff datetime\n",
    "    df[\"duration\"] = df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]\n",
    "    \n",
    "    # convert to minutes\n",
    "    df[\"duration\"] = df['duration'].dt.total_seconds() / 60\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the generated data are stored in the \"./data/\" directory. Consider the size of .csv files is too large, we git ignore *.csv files when we push to the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zachguan/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['dolocationid', 'extra', 'fare_amount', 'improvement_surcharge',\n",
       "       'mta_tax', 'passenger_count', 'payment_type', 'pulocationid',\n",
       "       'ratecodeid', 'tip_amount', 'tolls_amount', 'total_amount',\n",
       "       'tpep_dropoff_datetime', 'tpep_pickup_datetime', 'trip_distance',\n",
       "       'vendorid', 'duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./data/myData.csv\"\n",
    "\n",
    "df = read_from_csv(path)\n",
    "df = drop_useless_columns(df)\n",
    "df = convert_to_timestamp(df)\n",
    "df = create_duration_colum(df)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dolocationid</th>\n",
       "      <th>extra</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>ratecodeid</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>vendorid</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.80</td>\n",
       "      <td>2019-01-01 02:19:32</td>\n",
       "      <td>2019-01-01 02:19:27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>87.30</td>\n",
       "      <td>2019-01-01 05:52:53</td>\n",
       "      <td>2019-01-01 05:52:45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>80.30</td>\n",
       "      <td>2019-01-01 06:23:23</td>\n",
       "      <td>2019-01-01 06:22:24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>96.36</td>\n",
       "      <td>2019-01-01 08:13:01</td>\n",
       "      <td>2019-01-01 08:12:51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>160.80</td>\n",
       "      <td>2019-01-01 08:14:53</td>\n",
       "      <td>2019-01-01 08:14:48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2019-01-01 10:10:33</td>\n",
       "      <td>2019-01-01 10:10:30</td>\n",
       "      <td>16.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.26</td>\n",
       "      <td>16.56</td>\n",
       "      <td>2019-01-01 10:11:32</td>\n",
       "      <td>2019-01-01 10:10:52</td>\n",
       "      <td>16.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.26</td>\n",
       "      <td>16.56</td>\n",
       "      <td>2019-01-01 10:13:13</td>\n",
       "      <td>2019-01-01 10:12:05</td>\n",
       "      <td>16.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.26</td>\n",
       "      <td>136.56</td>\n",
       "      <td>2019-01-01 10:14:35</td>\n",
       "      <td>2019-01-01 10:13:44</td>\n",
       "      <td>16.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>18.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>108.36</td>\n",
       "      <td>2019-01-01 12:16:23</td>\n",
       "      <td>2019-01-01 12:16:18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dolocationid  extra  fare_amount  improvement_surcharge  mta_tax  \\\n",
       "0             1    0.0         20.0                    0.3      0.5   \n",
       "1             1    0.0         85.0                    0.3      0.0   \n",
       "2             1    0.0         80.0                    0.3      0.0   \n",
       "3             1    0.0         80.0                    0.3      0.0   \n",
       "4             1    0.0        160.0                    0.3      0.5   \n",
       "5             1    0.0          2.5                    0.3      0.5   \n",
       "6             1    0.0          0.0                    0.3      0.0   \n",
       "7             1    0.0          0.0                    0.3      0.0   \n",
       "8             1    0.0        120.0                    0.3      0.0   \n",
       "9             1    0.0         90.0                    0.3      0.0   \n",
       "\n",
       "   passenger_count  payment_type  pulocationid  ratecodeid  tip_amount  \\\n",
       "0                1             1             1           5        1.00   \n",
       "1                1             1             1           5        2.00   \n",
       "2                1             1             1           5        0.00   \n",
       "3                2             1             1           5       16.06   \n",
       "4                2             1             1           5        0.00   \n",
       "5                3             2             1           1        0.00   \n",
       "6                3             3             1           5        0.00   \n",
       "7                3             3             1           5        0.00   \n",
       "8                3             2             1           5        0.00   \n",
       "9                3             1             1           5       18.06   \n",
       "\n",
       "   tolls_amount  total_amount tpep_dropoff_datetime tpep_pickup_datetime  \\\n",
       "0          0.00         21.80   2019-01-01 02:19:32  2019-01-01 02:19:27   \n",
       "1          0.00         87.30   2019-01-01 05:52:53  2019-01-01 05:52:45   \n",
       "2          0.00         80.30   2019-01-01 06:23:23  2019-01-01 06:22:24   \n",
       "3          0.00         96.36   2019-01-01 08:13:01  2019-01-01 08:12:51   \n",
       "4          0.00        160.80   2019-01-01 08:14:53  2019-01-01 08:14:48   \n",
       "5          0.00          3.30   2019-01-01 10:10:33  2019-01-01 10:10:30   \n",
       "6         16.26         16.56   2019-01-01 10:11:32  2019-01-01 10:10:52   \n",
       "7         16.26         16.56   2019-01-01 10:13:13  2019-01-01 10:12:05   \n",
       "8         16.26        136.56   2019-01-01 10:14:35  2019-01-01 10:13:44   \n",
       "9          0.00        108.36   2019-01-01 12:16:23  2019-01-01 12:16:18   \n",
       "\n",
       "   trip_distance  vendorid  duration  \n",
       "0            0.0         2  0.083333  \n",
       "1            0.0         2  0.133333  \n",
       "2            0.0         2  0.983333  \n",
       "3            0.0         2  0.166667  \n",
       "4            0.0         2  0.083333  \n",
       "5           16.9         1  0.050000  \n",
       "6           16.9         1  0.666667  \n",
       "7           16.9         1  1.133333  \n",
       "8           16.9         1  0.850000  \n",
       "9            0.0         2  0.083333  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1838913"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_to_csv(df, \"./data/processed_myData.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the exploration and analysis of the column features and data distribution, we modify the original preprocessing methods. This will remove more data points (known as outliers or bad, ignorable rows) to make the whole dataset consistent and well-distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(df):\n",
    "    # drop rows with 0.0 total amount or 0.0 trip distance\n",
    "    df = df[(df['total_amount'] > 0.0) & (df['trip_distance'] > 0.0)]\n",
    "    \n",
    "    # trip duration should be less than or equal to 40 minutes\n",
    "    df = df[df['duration'] <= 90]\n",
    "    \n",
    "    # count of passengers should be positive and less than 5\n",
    "    df = df[(df['passenger_count'] > 0) & (df['passenger_count'] <= 4)]\n",
    "    \n",
    "    # trip distance between 1 and 20 miles\n",
    "    df = df[(df['trip_distance'] >= 1.) & (df['trip_distance'] <= 20)]\n",
    "    \n",
    "    # only reserve standard code trip\n",
    "    df = df[df['ratecodeid'] == 1]\n",
    "    \n",
    "    # get rid of payment type that is \"dispute\" or \"no charge\"\n",
    "    df = df[(df['payment_type'] == 1) | (df['payment_type'] == 2)]\n",
    "    \n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1196083"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = drop_outliers(df)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dolocationid</th>\n",
       "      <th>extra</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>ratecodeid</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>vendorid</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>2019-01-01 10:10:33</td>\n",
       "      <td>2019-01-01 10:10:30</td>\n",
       "      <td>16.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.30</td>\n",
       "      <td>2019-01-02 15:11:48</td>\n",
       "      <td>2019-01-02 14:50:55</td>\n",
       "      <td>7.92</td>\n",
       "      <td>2</td>\n",
       "      <td>20.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.5</td>\n",
       "      <td>60.30</td>\n",
       "      <td>2019-01-09 17:23:39</td>\n",
       "      <td>2019-01-09 16:55:48</td>\n",
       "      <td>14.97</td>\n",
       "      <td>2</td>\n",
       "      <td>27.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.30</td>\n",
       "      <td>2019-01-09 19:23:18</td>\n",
       "      <td>2019-01-09 19:13:21</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2</td>\n",
       "      <td>9.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.96</td>\n",
       "      <td>2019-01-11 07:52:27</td>\n",
       "      <td>2019-01-11 07:43:43</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2</td>\n",
       "      <td>8.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.30</td>\n",
       "      <td>2019-01-11 17:37:53</td>\n",
       "      <td>2019-01-11 17:37:51</td>\n",
       "      <td>14.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.80</td>\n",
       "      <td>2019-01-16 15:11:33</td>\n",
       "      <td>2019-01-16 14:59:21</td>\n",
       "      <td>3.31</td>\n",
       "      <td>2</td>\n",
       "      <td>12.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.30</td>\n",
       "      <td>2019-01-22 10:10:52</td>\n",
       "      <td>2019-01-22 09:32:00</td>\n",
       "      <td>11.20</td>\n",
       "      <td>1</td>\n",
       "      <td>38.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2019-01-25 16:35:23</td>\n",
       "      <td>2019-01-25 16:35:18</td>\n",
       "      <td>17.40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2019-01-25 16:37:27</td>\n",
       "      <td>2019-01-25 16:37:23</td>\n",
       "      <td>17.40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dolocationid  extra  fare_amount  improvement_surcharge  mta_tax  \\\n",
       "0             1    0.0          2.5                    0.3      0.5   \n",
       "1           265    0.0         24.5                    0.3      0.5   \n",
       "2           249    1.0         41.0                    0.3      0.5   \n",
       "3           237    1.0          8.5                    0.3      0.5   \n",
       "4           265    0.0          7.5                    0.3      0.5   \n",
       "5             1    1.0          2.5                    0.3      0.5   \n",
       "6             1    0.0         13.0                    0.3      0.5   \n",
       "7           265    0.0         38.5                    0.3      0.5   \n",
       "8             1    1.0          2.5                    0.3      0.5   \n",
       "9             1    1.0          2.5                    0.3      0.5   \n",
       "\n",
       "   passenger_count  payment_type  pulocationid  ratecodeid  tip_amount  \\\n",
       "0                3             2             1           1        0.00   \n",
       "1                1             2             1           1        0.00   \n",
       "2                1             2             1           1        0.00   \n",
       "3                1             1             1           1        1.00   \n",
       "4                1             1             1           1        1.66   \n",
       "5                4             1             1           1       39.00   \n",
       "6                1             2             1           1        0.00   \n",
       "7                1             2             1           1        0.00   \n",
       "8                1             2             1           1        0.00   \n",
       "9                1             2             1           1        0.00   \n",
       "\n",
       "   tolls_amount  total_amount tpep_dropoff_datetime tpep_pickup_datetime  \\\n",
       "0           0.0          3.30   2019-01-01 10:10:33  2019-01-01 10:10:30   \n",
       "1           0.0         25.30   2019-01-02 15:11:48  2019-01-02 14:50:55   \n",
       "2          17.5         60.30   2019-01-09 17:23:39  2019-01-09 16:55:48   \n",
       "3           0.0         11.30   2019-01-09 19:23:18  2019-01-09 19:13:21   \n",
       "4           0.0          9.96   2019-01-11 07:52:27  2019-01-11 07:43:43   \n",
       "5           0.0         43.30   2019-01-11 17:37:53  2019-01-11 17:37:51   \n",
       "6           0.0         13.80   2019-01-16 15:11:33  2019-01-16 14:59:21   \n",
       "7           0.0         39.30   2019-01-22 10:10:52  2019-01-22 09:32:00   \n",
       "8           0.0          4.30   2019-01-25 16:35:23  2019-01-25 16:35:18   \n",
       "9           0.0          4.30   2019-01-25 16:37:27  2019-01-25 16:37:23   \n",
       "\n",
       "   trip_distance  vendorid   duration  \n",
       "0          16.90         1   0.050000  \n",
       "1           7.92         2  20.883333  \n",
       "2          14.97         2  27.850000  \n",
       "3           2.01         2   9.950000  \n",
       "4           1.04         2   8.733333  \n",
       "5          14.60         1   0.033333  \n",
       "6           3.31         2  12.200000  \n",
       "7          11.20         1  38.866667  \n",
       "8          17.40         1   0.083333  \n",
       "9          17.40         1   0.066667  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is the way to take advantage of domain knowledge of the data to create new features that make machine learning algorithm works. This is very important in the data science project. Here we think of use feature engineering to improve the performance of metrics and develop better model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Features\n",
    "\n",
    "In this section, we first create some time related features which are derived from pickup time. We hypothesize that different day during a week or month or year and different hour in a day will have influence on trip duration. So we create following features:\n",
    "\n",
    "- hour_or_day: int [0-23] indicating which hour in the day of pickup time\n",
    "- day_of_week: indicating which day in the week, 0: Sunday, 1: Monday, 2: Tuesday, 3: Wednesday, 4: Thursday, 5: Friday, 6: Satureday\n",
    "- day_of_month: an integer from 1 to 30 or 31, indicating which day in the month\n",
    "- week_of_year: indicating which week in the year\n",
    "\n",
    "We also create a tip related feature which represent tip proportion of total amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hour_of_day\"] = df.tpep_dropoff_datetime.dt.hour\n",
    "df[\"day_of_week\"] = df.tpep_dropoff_datetime.dt.weekday\n",
    "df[\"day_of_month\"] = df.tpep_dropoff_datetime.dt.day\n",
    "df[\"week_of_year\"] = df.tpep_dropoff_datetime.dt.weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tip_proportion'] = df.tip_amount / df.total_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between Features\n",
    "\n",
    "Second, we use [variance inflation factor](https://en.wikipedia.org/wiki/Variance_inflation_factor) (VIF) and correlation heatmap to check whether there exist correlation between two independent variables. Then we will drop one of them to maintain independence between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all 1 column \"ratecodeid\" and datetime columns\n",
    "new_df = df.drop(columns=[\"tpep_dropoff_datetime\", \"tpep_pickup_datetime\", \"ratecodeid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = new_df.columns.tolist()\n",
    "columns.remove(\"duration\")\n",
    "features = \"+\".join(columns)\n",
    "\n",
    "target = \"duration\"\n",
    "y, X = patsy.dmatrices(target + '~' + features, new_df, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Features\"] = X.columns\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif = vif.T\n",
    "\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above VIF score for each feature, we notice that \"fare_amount\", \"total_amount\" have relatively high VIF score, which indicates they may have high correlation. Next we check the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between each pair of feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_score = new_df.corr(method ='pearson')\n",
    "pearson_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explicitly visualize the correlations by [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) from seaborn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15)) \n",
    "sns.heatmap(pearson_score,\n",
    "            xticklabels=pearson_score.columns,\n",
    "            yticklabels=pearson_score.columns,\n",
    "            annot=True)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that \"trip_distance\", \"total_amount\" and \"fare_amount\" have high correlation with each other. So we drop two of them to prevent from collinearity. Also, we should also drop the least important columns like \"dolocationid\" and \"pulocationid\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"total_amount\",\n",
    "                      \"tpep_dropoff_datetime\",\n",
    "                      \"tpep_pickup_datetime\",\n",
    "                      \"ratecodeid\",\n",
    "                      \"dolocationid\",\n",
    "                      \"pulocationid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "After the feature engineering step, we use [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to divide the processed dataframe into train set and test set two parts. In the **model.ipynb**, we will use the split data for validation and model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df, test_size=0.2)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "print(\"Train size: {}\".format(len(X_train)))\n",
    "print(\"Test size: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(X_train, \"./data/train.csv\")\n",
    "save_to_csv(X_test, \"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "In this notebook, we wish to explore the NYC taxi trip dataset further. We take the .csv file from initial preprocessing as input, and use statistical or visualization methods to have a close look at the column features. This will be helpful for us to modify the project direction in time, and also of great importance to select the fitting model for prediction step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the initial preprocessing, we simply drop some useless columns, convert data types like changing pickup and dropoff time from object to timestamp, create new **duration** feature, and drop the rows containing NaN. And we use these processed data for the data analysis step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/processed_myData.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, index_col=0)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Features\n",
    "\n",
    "Now, we will in turn explore the column features in given dataset. We intend to use some statistical methods, and several visualization tools we learnt in class to observe the distribution of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip Duration\n",
    "\n",
    "This is the very significant feature, and we plan to predict it first by using some regression models. Also, it's the key explanable variable for us to use if we wish to predict the total amount or tip percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dur_secs = pd.to_timedelta(df['duration']).dt.total_seconds()\n",
    "trip_dur_secs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the duration length of each trip, we could classify them into short trip (<15mins), medium trip (15~40mins) and long trip (>40mins). We use pie plot to see the percentage, and it will help determine which rows can be considered as outliers, when we want to do further data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dur_mins = trip_dur_secs / 60\n",
    "count_mins = Counter()\n",
    "\n",
    "\"\"\"\n",
    "short_trip: duration less than 20 minutes\n",
    "medium_trip: duration greater than or equal to 20 mins but less than 40 mins\n",
    "long trip: duration greater than or equal to 40 mins but less than 90mins\n",
    "others: duration longer than or equal ot 90mins\n",
    "\"\"\"\n",
    "for dur_mins in trip_dur_mins:\n",
    "    if dur_mins < 15:\n",
    "        count_mins['short_trip'] += 1\n",
    "    elif dur_mins < 40:\n",
    "        count_mins['medium_trip'] += 1\n",
    "    else:\n",
    "        count_mins['long_trip'] += 1\n",
    "\n",
    "print(count_mins)\n",
    "key_value_pairs = count_mins.items()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie([x[-1] for x in key_value_pairs], labels=[x[0] for x in key_value_pairs], explode=(0.1, 0.1, 0.1), autopct='%1.1f%%')\n",
    "ax.set_title('Trip Duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passenger Count\n",
    "\n",
    "Usually, the number of passengers for a yellow taxi is 1~6. But we find in the dataset, there exist some rows of which the passenger count is zero, and we should get rid of them. To make the data distribution more consistent, for those where the count is bigger than 4, we could gently ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger_count = df['passenger_count']\n",
    "passenger_count.plot.hist(bins=np.arange(10), color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger_count.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find some rows with 0 or more than 6 passengers, should get rid of them\n",
    "passenger_counter = Counter()\n",
    "for cnt in passenger_count:\n",
    "    if cnt == 0 or cnt >= 7:\n",
    "        continue\n",
    "    passenger_counter[cnt] += 1\n",
    "\n",
    "count_items = passenger_counter.items()\n",
    "print(count_items)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie([x[-1] for x in count_items], labels=[x[0] for x in count_items], explode=[0.2] * len(count_items), autopct='%1.1f%%')\n",
    "ax.set_title('Passenger Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip Distance\n",
    "\n",
    "In yellow taxi trip dataset, we are given the trip distance, instead of the pickup/dropoff longitude and latitude in green taxi trip dataset. Similar as we analyze the trip duration, this time we define the trip length based on the geographical distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dist = df['trip_distance']\n",
    "trip_dist.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dist.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to use box plots to look at the distance distribution in detail. Visualization by *boxplot* from pyplot will help us clearly see the numerical difference of trip distance in comparison to the pie plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dist_counter = defaultdict(list)\n",
    "\n",
    "for d in trip_dist:\n",
    "    # only consider trip distance less than 30 miles\n",
    "    if d < 5:\n",
    "        trip_dist_counter['<5'].append(d)\n",
    "    elif d < 10:\n",
    "        trip_dist_counter['5-10'].append(d)\n",
    "    elif d < 20:\n",
    "        trip_dist_counter['10-20'].append(d)\n",
    "    elif d <= 30:\n",
    "        trip_dist_counter['<30'].append(d)\n",
    "\n",
    "dist_items = sorted(trip_dist_counter.items(), key=lambda x:x[-1])\n",
    "plt.boxplot([x[-1] for x in dist_items], labels=[x[0] for x in dist_items])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dist_counter2 = {}\n",
    "for k in trip_dist_counter:\n",
    "    trip_dist_counter2[k] = len(trip_dist_counter[k])\n",
    "\n",
    "print(trip_dist_counter2)\n",
    "    \n",
    "count_items = trip_dist_counter2.items()\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie([x[-1] for x in count_items], labels=[x[0] for x in count_items], explode=[0.2] * len(count_items), autopct='%1.1f%%')\n",
    "ax.set_title('Trip Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do preprocessing such as using log-normed values. This makes a lot more sense, since continuous probability distribution is prone to perform better if we do linear regression or other advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_dist_mean = trip_dist.mean()\n",
    "trip_dist_std = trip_dist.std()\n",
    "\n",
    "# remove outliers based on 3-sigma rule\n",
    "trip_dist = trip_dist[(trip_dist - trip_dist_mean).abs() < 3 * trip_dist_std]\n",
    "trip_dist.plot.hist()  # should focus on trips with distance less than 8 miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-normal fit\n",
    "shape, location, scale = lognorm.fit(trip_dist, scale=trip_dist_mean, loc=0)\n",
    "normed = lognorm.pdf(np.arange(9), shape, location, scale)\n",
    "plt.plot(np.arange(9), normed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate Code\n",
    "\n",
    "The meaning of 'ratecodeid' column is the final rate code in effect at the end of the trip. The mapping relationships are listed in the following 'code_map'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_code = df['ratecodeid']\n",
    "rate_code_counter = Counter(rate_code)\n",
    "print(rate_code_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_map = {\n",
    "    1: 'Standard',\n",
    "    2: 'JFK',\n",
    "    3: 'Newark',\n",
    "    4: 'Nassau or Westchester',\n",
    "    5: 'Negotiated Fare',\n",
    "    6: 'Group Ride',\n",
    "    99: 'others'\n",
    "}\n",
    "\n",
    "total = sum(rate_code_counter.values())\n",
    "for k, v in rate_code_counter.items():\n",
    "    percentile = 100 * v / total\n",
    "    print(\"{}% trips are {}\".format(round(percentile, 3), code_map[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate_code = pd.DataFrame(rate_code_counter.values(), index=rate_code_counter.keys())\n",
    "ax = df_rate_code.plot.bar(rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Payment Types\n",
    "\n",
    "For the payment method, there are totally four different types. Customers mainly use credit card, and less than one third bring cash. Thus, we merely select the data where payment type code is 1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_types = {1: \"Credit card\", 2: \"Cash\", 3: \"No charge\", 4: \"Dispute\"}\n",
    "x = [payment_types[idx] for idx in list(df[\"payment_type\"].value_counts().index)]\n",
    "y = list(df[\"payment_type\"].value_counts().values)\n",
    "\n",
    "dic = {}\n",
    "for i in range(len(x)):\n",
    "    dic[x[i]] = y[i]\n",
    "print(dic)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(y, labels=x, explode=(0.1, 0.1, 0.1, 0.1), autopct='%1.1f%%')\n",
    "ax.set_title('Payment Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fare Amount\n",
    "\n",
    "Likewise, we use histogram and log-normed curve to explore the fare amount column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-672b2d2eef09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfare_amount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fare_amount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfare_amount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fare_amount\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of Rides\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "fare_amount = df['fare_amount']\n",
    "ax = fare_amount.hist(bins=100)\n",
    "ax.set_xlabel(\"fare_amount\")\n",
    "ax.set_ylabel(\"Number of Rides\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fare_amount' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-97fcc32228da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfare_amount_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfare_amount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfare_amount_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfare_amount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfare_amount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfare_amount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfare_amount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfare_amount_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfare_amount_std\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfare_amount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fare_amount' is not defined"
     ]
    }
   ],
   "source": [
    "fare_amount_mean = fare_amount.mean()\n",
    "fare_amount_std = fare_amount.std()\n",
    "fare_amount = fare_amount[(fare_amount - fare_amount_mean).abs() < 3 * fare_amount_std]\n",
    "fare_amount.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape, location, scale = lognorm.fit(fare_amount, scale=fare_amount_mean, loc=0)\n",
    "normed = lognorm.pdf(np.arange(100), shape, location, scale)\n",
    "plt.plot(np.arange(100), normed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these 'amount' related columns, their values sum up to the 'total amount' in dataframe. They can be taken as bool or categorical type of data, which are also important in duration prediction.\n",
    "\n",
    "## Tolls Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df['tolls_amount'].hist(bins=100)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"tolls_amount\")\n",
    "ax.set_ylabel(\"Number of Rides\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surcharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [str(i) for i in list(df[\"improvement_surcharge\"].value_counts().index)]\n",
    "y = list(df[\"improvement_surcharge\"].value_counts().values)\n",
    "\n",
    "plt.bar(labels, y, align='center')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "\n",
    "for a,b in zip(labels, y):\n",
    "    plt.text(a, b, str(b))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['extra'].describe())\n",
    "\n",
    "ax = df['extra'].hist(bins=20)\n",
    "ax.set_xlabel(\"extra\")\n",
    "ax.set_ylabel(\"Number of Rides\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data\n",
    "\n",
    "We take the splited train and test dataset generated from the **preprocess.ipynb** as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_csv(path):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/train.csv'\n",
    "test_path = './data/test.csv'\n",
    "\n",
    "X_train, X_test = read_from_csv(train_path, ), read_from_csv(test_path)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = pd.DataFrame(X_train['duration']), pd.DataFrame(X_test['duration'])\n",
    "\n",
    "X_train.drop(columns=['duration'], inplace=True)\n",
    "X_test.drop(columns=['duration'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we apply the [root-mean-square error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE) as the metric for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "\n",
    "Our task is to build a regression model to predict the duration of a yellow taxi trip. We apply linear regression model as our baseline model. To capture the non-linear relationship between features and target, we further use XgBoost, Gradient Boost, Random Forest and Neural Networks for prediction. We implement parameter searching and tuning for those models to get the best results. We take root mean square error as evaluation metric. Finally, we plan to compare and analyze among different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "For linear regression model, we choose four different variations. The simplest version of linear model shows as follow:\n",
    "$$ \\hat y = w_{1}x_{1} + w_{1}x_{1} + ... w_{n}x_{n} + b $$\n",
    "\n",
    "The cost function is:\n",
    "$$ \\sum_{i=1}^{N} (y_{i} - w_{i}^{T} x_{i}) $$\n",
    "\n",
    "For Ridge regression, the cost function consists of loss and regularization part. Regularization part equals to square of the magnitude of the coefficients:\n",
    "$$ \\sum_{i=1}^{N} (y_{i} - w_{i}^{T} x_{i}) + \\lambda \\sum_{j=0}^{F}w_{j}^{2}$$\n",
    "\n",
    "For Lasso regression, regularization part of cost function is sum of absolute value of weights. This L1 regularization can shrink and completely neglect some of the features:\n",
    "$$ \\sum_{i=1}^{N} (y_{i} - w_{i}^{T} x_{i}) + \\lambda \\sum_{j=0}^{F}|w_{j}|$$\n",
    "\n",
    "For [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) regression, it combines both L1 and L2 regularization into cost function.\n",
    "\n",
    "Through the test, we find normalization will make performance worse, so we choose to use the default \"normalize=False\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/linear_model.html\n",
    "model_lr = LinearRegression().fit(X_train, y_train)\n",
    "model_ridge = Ridge().fit(X_train, y_train)\n",
    "model_lasso = Lasso().fit(X_train, y_train)\n",
    "model_elasticnet = ElasticNet().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "y_pred1 = model_lr.predict(X_train)\n",
    "y_pred2 = model_ridge.predict(X_train)\n",
    "y_pred3 = model_lasso.predict(X_train)\n",
    "y_pred4 = model_elasticnet.predict(X_train)\n",
    "\n",
    "train_loss1 = get_rmse(y_train, y_pred1)\n",
    "train_loss2 = get_rmse(y_train, y_pred2)\n",
    "train_loss3 = get_rmse(y_train, y_pred3)\n",
    "train_loss4 = get_rmse(y_train, y_pred4)\n",
    "\n",
    "print(\"Train Loss for Linear Regression: {}\".format(train_loss1))\n",
    "print(\"Train Loss for Ridge Regression: {}\".format(train_loss2))\n",
    "print(\"Train Loss for Lasso Regression: {}\".format(train_loss3))\n",
    "print(\"Train Loss for Elastic_Net Regression: {}\".format(train_loss4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "y_pred1 = model_lr.predict(X_test)\n",
    "y_pred2 = model_ridge.predict(X_test)\n",
    "y_pred3 = model_lasso.predict(X_test)\n",
    "y_pred4 = model_elasticnet.predict(X_test)\n",
    "\n",
    "test_loss1 = get_rmse(y_test, y_pred1)\n",
    "test_loss2 = get_rmse(y_test, y_pred2)\n",
    "test_loss3 = get_rmse(y_test, y_pred3)\n",
    "test_loss4 = get_rmse(y_test, y_pred4)\n",
    "\n",
    "print(\"Test Loss for Linear Regression: {}\".format(test_loss1))\n",
    "print(\"Test Loss for Ridge Regression: {}\".format(test_loss2))\n",
    "print(\"Test Loss for Lasso Regression: {}\".format(test_loss3))\n",
    "print(\"Test Loss for Elastic_Net Regression: {}\".format(test_loss4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot output, take \"model_lr\" as example\n",
    "plt.scatter(np.arange(X_test.shape[0]), y_test, color='r', s=10)\n",
    "plt.plot(np.arange(X_test.shape[0]), y_pred1, color='b', linewidth=0.1)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Xgboost uses K additive functions to predict the output. The output is sum of scores predicted by each of the tree. Instead learning weights like in linear regression, XgBoost is learning functions (trees) in each iteration.\n",
    "\n",
    "The objective funcion consists of train loss and regularization part. The training process starts from constant output. At each time add a new function which minimize the above objective function:\n",
    "\n",
    "$$ Obj^{t} = \\sum_{i=1}^{n} l(y_{i}, \\hat {y_{i}}^{t-1}+f_{t}(x_{i})) + \\Omega (f_{t}) \\quad f_{t} \\subseteq F \\, (all \\, regression \\, trees) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { \n",
    "    'booster': 'gbtree',\n",
    "    'objective':'reg:linear',\n",
    "    'learning_rate': 0.2,\n",
    "    'n_estimators': 200,\n",
    "    'objective': 'reg:linear',  \n",
    "    'gamma': 0.3,                  # control pruning\n",
    "    'max_depth':5 ,               \n",
    "    'lambda': 2,                   # L2 parameter\n",
    "    'subsample': 0.8,              # random sample \n",
    "    'colsample_bytree': 0.7,       # col sample when generate tree\n",
    "    'min_child_weight': 1,\n",
    "    'silent': 0,\n",
    "    'reg_alpha': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgboost = xgb.XGBRegressor(\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        booster=params['booster'],\n",
    "        objective=params['objective'],\n",
    "        n_jobs=-1,\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        random_state=0,\n",
    "        silent=params['silent'],\n",
    "        max_depth=params['max_depth'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        reg_alpha=params['reg_alpha']\n",
    "    )\n",
    "\n",
    "model_xgboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "y_pred = model_xgboost.predict(X_train)\n",
    "train_loss = get_rmse(y_train, y_pred)\n",
    "print(\"Train Loss: {}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "y_pred = model_xgboost.predict(X_test)\n",
    "test_loss = get_rmse(y_test, y_pred)\n",
    "print(\"Test Loss: {}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Parameters for Xgboost\n",
    "\n",
    "Considering time complexity and effect on result, we choose \"learning rate\", \"number of estimators\" and \"gamma\" as tunning parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'booster': 'gbtree',\n",
    "        'objective':'reg:squarederror',\n",
    "        'max_depth': 5,\n",
    "        'lambda': 2,                   # L2 parameter\n",
    "        'subsample': 0.8,              # random sample\n",
    "        'colsample_bytree': 0.7,       # col sample when generate tree\n",
    "        'min_child_weight': 1,\n",
    "        'reg_alpha': 0,\n",
    "        'verbosity':1\n",
    "    }\n",
    "\n",
    "search_params={\n",
    "        'learning_rate': [0.1, 0.2, 0.3],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'gamma': [1, 2, 5, 10],\n",
    "    }\n",
    "\n",
    "model_xgboost = xgb.XGBRegressor(\n",
    "        booster=params['booster'],\n",
    "        objective=params['objective'],\n",
    "        n_jobs=-1,\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        random_state=0,\n",
    "        max_depth=params['max_depth'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        verbosity=params['verbosity']\n",
    "    )\n",
    "\n",
    "cv_folders = 5\n",
    "gs = GridSearchCV(model_xgboost, search_params, scoring=\"neg_mean_absolute_error\", cv=cv_folders, verbose=2)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.scorer_, gs.best_estimator_, gs.best_params_, gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model: gs.best_estimator_\n",
    "best_xgboost = xgb.XGBRegressor(\n",
    "                base_score=0.5,\n",
    "                booster='gbtree',\n",
    "                colsample_bylevel=1,\n",
    "                colsample_bynode=1,\n",
    "                colsample_bytree=0.7,\n",
    "                gamma=5,\n",
    "                importance_type='gain',\n",
    "                learning_rate=0.2,\n",
    "                max_delta_step=0,\n",
    "                max_depth=5,\n",
    "                min_child_weight=1,\n",
    "                missing=None,\n",
    "                n_estimators=300,\n",
    "                n_jobs=-1,\n",
    "                nthread=None,\n",
    "                objective='reg:squarederror',\n",
    "                random_state=0,\n",
    "                reg_alpha=0,\n",
    "                reg_lambda=1,\n",
    "                scale_pos_weight=1,\n",
    "                seed=None,\n",
    "                silent=None,\n",
    "                subsample=0.8,\n",
    "                verbosity=1)\n",
    "\n",
    "best_xgboost.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_xgboost.predict(X_train)\n",
    "train_loss_xgboost = get_rmse(y_train, y_pred)\n",
    "y_pred = model_xgboost.predict(X_test)\n",
    "test_loss_xgboost = get_rmse(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_xgboost.feature_importances_)\n",
    "xgb.plot_importance(best_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Random forest builds on bagging by random selecting samples, features and getting result from random set of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor(n_jobs=-1, max_depth=10, verbose=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "y_pred = model_rf.predict(X_train)\n",
    "train_loss_rf = get_rmse(y_train, y_pred)\n",
    "print(\"Train Loss: {}\".format(train_loss_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "y_pred = model_rf.predict(X_test)\n",
    "test_loss_rf = get_rmse(y_test, y_pred)\n",
    "print(\"Test Loss: {}\".format(test_loss_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "estimator = model_rf.estimators_[1]\n",
    "\n",
    "from IPython.display import SVG\n",
    "graph = Source(export_graphviz(estimator, out_file=None, feature_names=X_train.columns))\n",
    "SVG(graph.pipe(format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "importances = model_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "columns = X_train.columns.tolist()\n",
    "\n",
    "plt.title(\"Feature importances\", fontsize=14)\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"b\",align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), [columns[idx] for idx in indices], rotation=75, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost\n",
    "\n",
    "The objective of gradient boost is to minimize the loss of the model by adding decision tree one at a time using gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb = GradientBoostingRegressor(verbose=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "y_pred = model_gb.predict(X_train)\n",
    "train_loss_gb = get_rmse(y_train, y_pred)\n",
    "print(\"Train Loss: {}\".format(train_loss_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "y_pred = model_gb.predict(X_test)\n",
    "test_loss_gb = get_rmse(y_test, y_pred)\n",
    "print(\"Test Loss: {}\".format(test_loss_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/getting-started/sequential-model-guide/\n",
    "model_nn = models.Sequential()\n",
    "model_nn.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model_nn.add(BatchNormalization())\n",
    "model_nn.add(Dense(64, activation='relu'))\n",
    "model_nn.add(BatchNormalization())\n",
    "model_nn.add(Dense(32, activation='relu'))\n",
    "model_nn.add(BatchNormalization())\n",
    "model_nn.add(Dense(8, activation='relu'))\n",
    "model_nn.add(BatchNormalization())\n",
    "model_nn.add(Dense(1))\n",
    "\n",
    "optimizer = optimizers.Adam(lr=1e-4)\n",
    "model_nn.compile(loss='mse', optimizer=optimizer, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_nn.fit(\n",
    "                x=X_train,\n",
    "                y=y_train,\n",
    "                validation_data=(X_test, y_test), \n",
    "                batch_size=256,\n",
    "                epochs=8,\n",
    "                shuffle=True,\n",
    "                verbose=1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_nn = history.history['loss']\n",
    "test_loss_nn = history.history['val_loss']\n",
    "epochs = np.arange(len(train_loss_nn))\n",
    "\n",
    "plt.plot(epochs, train_loss_nn, 'r')\n",
    "plt.plot(epochs, test_loss_nn, 'b')\n",
    "plt.legend(['Train Loss', 'Test Loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train RMSE score for the Linear Regression model is : %f'%(train_loss1))\n",
    "print('Train RMSE score for the Ridge model is : %f'%(train_loss2))\n",
    "print('Train RMSE score for the Lasso model is : %f'%(train_loss3))\n",
    "print('Train RMSE score for the ElasticNet model is : %f'%(train_loss4))\n",
    "print('Train RMSE score for the Random Forest model is : %f'%(train_loss_rf))\n",
    "print('Train RMSE score for the Gradient Boost model is : %f'%(train_loss_gb))\n",
    "print('Train RMSE score for the Xgboost model is : %f'%(math.sqrt(train_loss_xgboost)))\n",
    "print('Train RMSE score for the Neural Network is : %f'%(math.sqrt(train_loss_nn[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test RMSE score for the Linear Regression model is : %f'%(test_loss1))\n",
    "print('Test RMSE score for the Ridge model is : %f'%(test_loss2))\n",
    "print('Test RMSE score for the Lasso model is : %f'%(test_loss3))\n",
    "print('Test RMSE score for the ElasticNet model is : %f'%(test_loss4))\n",
    "print('Test RMSE score for the Random Forest model is : %f'%(test_loss_rf))\n",
    "print('Test RMSE score for the Gradient Boost model is : %f'%(test_loss_gb))\n",
    "print('Test RMSE score for the Xgboost model is : %f'%(math.sqrt(test_loss_xgboost)))\n",
    "print('Test RMSE score for the Neural Network is : %f'%(math.sqrt(test_loss_nn[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this project, we extract yellow taxi data from NYC Open Data, conduct data preprocessing, data analysis, feature engineering and implement a series of models to predict taxi trip duration. Based on our experiments, we obtain the following conclusions:\n",
    "\n",
    "- Linear regression and its variations are treated as our baseline model. On both train set and test set, they get higher RMSE score. Among simple Linear Regression, Ridge, Lasso and Elastic Net, Ridge regression has slightly better performance over the test set.\n",
    "\n",
    "- Compared with Linear Regression, Random Forest model has around 15% lower RMSE both on train set and test set.\n",
    "\n",
    "- Performance of Gradient Boost model is worse than random forest which may cause from the inappropriate selection of some hyper-parameters. \n",
    "\n",
    "- Xgboost model achieves lowest RMSE score on train set and test set. It is an optimized distributed Gradient Boosting method which leads to both greater metrics performance and less time cost.\n",
    "\n",
    "During the whole project, we experience the full data science development cycle. We notice the importance of data exploration and data cleaning before building models. And after understanding data distribution and building baseline, we furthermore improve our feature selection based on the baseline results until we get the optimal features. Finally, we utilize optimal feature space to construct model and achieve the best result. For future work, we will go deeper with the algorithm in each model and try to apply them in different aspects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
